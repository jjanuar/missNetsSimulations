---
title: "20211130_simu_trials"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Here's a first pass at simulating some networks, probably Bernoulli ones, and figuring out how I'm going to generate some missing data indicators (also probably some Bernoulli related thing?)

```{r packages, eval = T, warning = F, error = F, message = F}
# some packages
library(sna)
library(ergm)
```


## Some resources

[This stats exchange](https://stats.stackexchange.com/questions/184741/how-to-simulate-the-different-types-of-missing-data) describes a way to simulate MCAR/MAR/MNAR with some wrangling tricks, but I'm not sure how to apply this to networks.

[Huisman(2009)](https://www.cmu.edu/joss/content/articles/volume10/huisman.pdf) goes over a general way to investigate missing data in networks, but no code AFAIK.


## Item non-response, directed network

**Note:** As missing values can either be 0s or 1s, subsetting only the observable ties to degrade would be a moot point. Degrade the possible tie variables as well (i.e., they would be 0s too).

Let's start with generating a Bernoulli graph

```{r berngraph}
## ---- fix a seed FOR NOW for some replicability
set.seed(123)

# using the rgraph() function
testGraph = rgraph(n = 15,            # 15 nodes
                   m = 1,             # 1 generated graph
                   tprob = 0.5,       # tie probability/density of 0.5
                   mode = 'digraph')  # directed for now

## how do I even start with degrading this adjacency matrix?
# first let's start with a sum of ties
sum(testGraph)

# let's see it
gplot(testGraph, xlab = "Test")

# if it's undirected, divide by 2 for the number of unique ties.

# set a proportion of missingness
propMiss = 0.1                        # say 10% of ties are missing.

# I can very roughly use a uniform distribution to generate # of ties to be thresholded with the missing proportion
# e.g.,
missThresh = matrix(data = runif(15^2, min = 0, max = 1),
                    nrow = 15,
                    ncol = 15)

# TODO: not sure how I'm going to handle unit non-response (completely missing actor/node and thus completely missing their ties)

# on the other hand, item non-response should be fairly straightforward (e.g., missing responses from observed actor)

# index which ties are going to be missing
missTies = missThresh <= propMiss

# then we degrade
degradedGraph = testGraph             # copy the network to the one we're going to wreck

# and we punch holes in it
degradedGraph[missTies] = NA 

# let's see the plot
gplot(degradedGraph, xlab = "Degraded")

# side by side?
par(mfrow = c(1,2))   # 1 row, 2 columns
gplot(testGraph, xlab = "Test")
gplot(degradedGraph, xlab = "Degraded")


##---it's hard to see an effect, but let's try replicating the code to a higher missingness proportion---

# set a proportion of missingness
propMiss = 0.5                        # say 50% of ties are missing.

# use the same missing threshold matrix
# index which ties are going to be missing
missTies = missThresh <= propMiss

# then we degrade
degradedGraph = testGraph             # copy the network to the one we're going to wreck

# and we punch holes in it
degradedGraph[missTies] = NA 

# let's see the plot
gplot(degradedGraph, xlab = "Degraded, propMiss = 0.5")

# side by side?
par(mfrow = c(1,2))   # 1 row, 2 columns
gplot(testGraph, xlab = "Test")
gplot(degradedGraph, xlab = "Degraded")


```

Seems like the effect is a lot more apparent now. 

## Item non-response, undirected network

Undirected networks need to be symmetric, so I think I can shrink the degradation to a triangle, doesn't matter if upper or lower, and then symmetrise the missingness afterwards. 

```{r undirected missnet}
## basically replicating the previous code examples, but for undirected networks
# I think the real question's going to be how I'm going to make a function NOT use an if branch

# first, simulate an undirected random graph
testUndGraph = rgraph(n = 15,
                      m = 1,
                      tprob = 0.5,
                      mode = "graph")      # undirected

# check plot
gplot(testUndGraph,
      xlab = "Test undirected graph",
      gmode = "graph")

# move the test graph into a degraded graph placeholder
degradedUndGraph = testUndGraph

# let's use the same missingness threshold and matrix
# this atrocious line of code replaces the upper triangle of the undirected graph with missing values
degradedUndGraph[upper.tri(testUndGraph)][missTies[upper.tri(missTies)]] = NA

# symmetrise
degradedUndGraph[lower.tri(degradedUndGraph)] = 0           # empty lower triangle
degradedUndGraph = degradedUndGraph + t(degradedUndGraph)   # add the transpose

# check
isSymmetric(degradedUndGraph)

gplot(degradedUndGraph, 
      xlab = "Degraded undirected graph",
      gmode = "graph")

# side by side
par(mfrow = c(1,2))
gplot(testUndGraph,
      xlab = "Test undirected graph",
      gmode = "graph")
gplot(degradedUndGraph, 
      xlab = "Degraded undirected graph",
      gmode = "graph")

```


## degradeNet function

```{r functionalDegradation}
## let's try turning the network degradation into a function so it's easier to recursively do.

degradeNet <- function(graph, propMiss, directed = TRUE){
  
  ## degradeNet(graph, propMiss) takes a graph and replaces the observed graph ties with missing 
  ## values(NA). This function does not handle actor non-response as it only works off the observed
  ## network ties. Works with undirected networks.
  ##
  ## Input:
  ## - graph:    An adjacency matrix describing a graph. There shouldn't
  ##             be any missing values in this adjacency matrix yet.
  ##             Should still work if there were though. I think.
  ## - propMiss: A numeric value to indicate the proportion of missingness
  ##             that will be imposed on the observed network.
  ##             Is bounded between 0 and 1.
  ## - directed: A logical value to indicate if it's a directed or
  ##             undirected network. Default is set to directed.
  ##
  ## Output: 
  ## - A degraded network with missing ties where observed ties once were.
  ##   Note that it only handles item non-response for now. Given the inputs,
  ##   the missing values would be ~'propMiss'% of the observed tie variables in 'graph'.
  
  ## spit out an error if the directed argument is misspecified
  if(directed != TRUE & !isSymmetric(graph)){
    stop("The undirected network doesn't have a symmetric matrix")
  }
  
  ## spit out an error if the proportion of missingness exceeds bounds
  if(propMiss > 1 | propMiss < 0){
    stop("The proportion of missingness exceeds bounds")
  }
  
  # grab the number of nodes
  n = nrow(graph)
  
  # make a n x n matrix containing randomly generated values between 0 and 1
  missThresh = matrix(data = runif(n^2, min = 0, max = 1),
                      nrow = n,
                      ncol = n)
  
  # index which of the ties are going to be missing
  missTies = missThresh <= propMiss
  
  # copy the graph to punch holes in it
  degradedGraph = graph
  
  if( directed == FALSE ){
    # this atrocious line of code replaces the upper triangle of the undirected graph with missing values
    degradedGraph[upper.tri(graph)][missTies[upper.tri(missTies)]] = NA

    # symmetrise
    degradedGraph[lower.tri(degradedGraph)] = 0           # empty lower triangle
    degradedGraph = degradedGraph + t(degradedGraph)      # add the transpose
  } else {
    # punch holes
    degradedGraph[missTies] = NA
  }
  
  # return the degraded graph
  return(degradedGraph)
}

```


```{r testingDegradeNet}
# test the function
missNet1 = degradeNet(graph = testGraph,
                      propMiss = 0.2)
missNet2 = degradeNet(graph = testGraph,
                      propMiss = 0.7)

# some statistics?
meanDeg = data.frame(sum(testGraph, na.rm = T)/nrow(testGraph), sum(missNet1, na.rm = T)/nrow(missNet1), sum(missNet2, na.rm = T)/nrow(missNet2))
colnames(meanDeg) = c("Test", "propMiss = 0.2", "propMiss = 0.7")
print(meanDeg)

# compare plots
par(mfrow = c(1, 3))      # 1 row, 3 columns
gplot(testGraph, xlab = "Test")
gplot(missNet1, xlab = "Directed, propMiss = 0.2")
gplot(missNet2, xlab = "Directed, propMiss = 0.7")

## And the same for undirected networks?
missUndNet1 = degradeNet(graph = testUndGraph,
                         propMiss = 0.3,
                         directed = FALSE)
missUndNet2 = degradeNet(graph = testUndGraph,
                         propMiss = 0.6,
                         directed = FALSE)
# some statistics
meanDeg = data.frame(sum(testUndGraph, na.rm = T)/nrow(testUndGraph), 
                     sum(missUndNet1, na.rm = T)/nrow(missUndNet1),
                     sum(missUndNet2, na.rm = T)/nrow(missUndNet2))
colnames(meanDeg) = c("Test", "propMiss = 0.3", "propMiss = 0.6")
print(meanDeg)

# compare plots
par(mfrow = c(1, 3))      # 1 row, 3 columns
gplot(testUndGraph,
      xlab = "Test Undirected",
      gmode = "graph")
gplot(missUndNet1, 
      xlab = "Undirected, propMiss = 0.3",
      gmode = "graph")
gplot(missUndNet2,
      xlab = "Undirected, propMiss = 0.6",
      gmode = "graph")

```

Given that I used runif() to randomly choose a threshold for missingness, this is quite literally missing completely at random. 

Nonetheless, the degraded networks seem to work fine. Now for the part to make its missing data generation more complicated.

**Note:** *Why both observed and non-observed ties?* Because missing ties do not discriminate. An example would be non-respondents. If we know A sends ties to B, and sends ties to C, it might be likely that C reciprocates the ties (as per triadic closure). However, if C does not respond, we can't say that tie exists or not.

*Are there any cases where we'd want to only degrade observed ties?* Hmm. Maybe in a MAR case where we know that a portion of observed ties (perhaps due to a covariate) should have ties, but show up as missing for some reason. For example, perhaps if we question a bunch of people from different ages, the youngest ones would not be able to respond (e.g., whether they're connected to a far-away uncle or something), but we know they're connected by sources not immediately visible from the collected data.

## Comparing statistics across networks

We can calculate statistics like this and compare:

```{r statCalcs}
## an example with mean degree
sum(testGraph)/nrow(testGraph)

# compared with the degraded network
sum(degradedGraph, na.rm = T)/nrow(degradedGraph)

# a notable decrease
# and with this 'systematic' (not really, but we can control the proportion of missingness) change, we can plot distributions.

# but I'll leave this proof of concept here for the time being (can be extended to any network statistic)
# and try make the code a bit more functional.

## An ERGM can be fit to the degraded network to calculate some network statistics.
# it's just implied that the missingess is MAR.
testErgm1 = ergm(testGraph ~ edges + 
                   mutual)
summary(testErgm1)

# and a degraded graph
degradedErgm1 = ergm(degradedGraph ~ edges+
                       mutual)
summary(degradedErgm1)

```

## More nuanced missingness

Here's the part that tries to escape the mathematically easy realm of MCAR (and runif()) and tries to bring in some systematic way to choose missing data indicators.

I'm not entirely sure how to do this.

What do I know?

We're generating Bernoulli graphs, so each possible tie is a Bernoulli trial.

That means that the in and out degree distributions would be $\sim Bi(Nodes, density)$, with 15 nodes and a tie probability (or density) of 0.5, the distribution would be $\sim Bi(15, 0.5)$. *Can I use this somehow??*

The goal right now is to generate missing data points in a more systematic way...

So! Given that the missing data indicator (That would be the 'missTies' object in this script) is extremely similar to an adjacency matrix (it's a logical matrix, but that's unimportant), this might mean that we can impose a model to generate a network (adjacency matrix) to indicate missing data points!




```{r moreMissingness}
# umm... Let's start by seeing the degree distributions for the test graph?
# let's do directed first.
hist(colSums(testGraph), main = "Indegree distribution")
hist(rowSums(testGraph), main = "Outdegree distribution")

# they should both be roughly ~Bi(15,0.5).


```


Let $D_{i \times j}$ represent a missing data indicator matrix for adjacency matrix $X$.

$$d_{ij} = \begin{cases} 0 & \text{if } { x_{ij} = 0,1}\\ 1 & \text{if }{x_{ij}} = \text{missing}\\ \end{cases}$$

$d_{ij} = 1$ only when the edge $x_{ij}$ is missing.

This format of data is very similar to an adjacency matrix $X$

$$x_{ij} = \begin{cases} 0 & \text{if } i = j\\ 1 & \text{if } i \text{ sends a tie to } j\\ \end{cases}$$

Also note, $X = x_{ij}: i, j \in \{1, ..., n\}$ for $n$ participants.

Then, we can apply models for adjacency matrices (networks) to missing data indicator $D$.

$$D|X \sim \text{Some network model}$$

One of these network models being an ERGM.

$$P(D|X) = \prod_{ij} \frac{\exp(\alpha + \beta x_{ij} d_{ij})}{1 + \exp(\alpha + \beta x_{ij}d_{ij})}$$

Which if you plug into a logit function,

$$logit(P(D|X)) = \alpha + \beta x_{ij} d_{ij}$$

And if you were to extend it to the (log) odds of a missing tie,

$$logit(\frac{Pr(d_{ij}= 1| x_{ij} = 1)}{Pr(d_{ij} = 1 | x_{ij} = 0)}) = log(\frac{e^{\alpha + \beta}}{e^\alpha})$$

$$ = \beta$$

So $\beta$ would represent the probability (?) of a missing tie given an observed tie against an unobserved tie.


This is a case with a simple generalised linear model (logistic reg.), extending it to the ERGM would be like

$$P(D| X, \alpha, \beta) = \frac{exp(\alpha \sum d_{ij} + \beta \sum x_{ij}d_{ij})}{\phi(\alpha, \beta)}$$

Where phi's the normalising constant for the model given its model parameters.

**Note:** How is this different to a Bayesian approach to the ERGM?

AFAIK the Bayesian approach estimates the missingness wrt the observed ties and model parameters.

$$P(X_{mis} | X_{obs}, \theta)$$

Note that the $\theta$ here is the same $\theta$ being used to estimate the observed ties $P(X_{obs} | X_{mis}, \theta)$

So an 'advantage' (?) would be that modelling $D$ could use a different set of parameters than the Bayesian model.

That is, $\theta$ in $P(X_{mis}|X_{obs}, \theta)$ is the same as $P(X_{obs}|X_{mis}, \theta)$ whereas $P(D_{ij}|X_{ij}, \theta)$ can be different to $P(X_{ij}|D_{ij}, \theta)$.

I don't necessarily think this is particularly useful or computationally efficient, but I do think that modelling the missing tie indicators separately to modelling the observable adjacency matrix allows for more specialised modelling.

e.g., suppose modelling a particular outcome variable is desired when modelling the observable adjacency matrix, however we might want to avoid imputing missing ties if we account for that particular outcome variable.

You could also just do multiple Bayesian ERGMs with different specifications, but well, I'm still not entirely sure how to use a model for $D_{ij}$.

The Bayesian ERGM can be written as

$$P(X_{mis} | X_{obs}, D, \theta, \eta) = P(X_{mis}, X_{obs} | \theta)P(D|X, \eta)$$

where

$\theta$ represents the model parameters for adjacency matrix $X$.

$\eta$ represents the model parameters relating $D$ to $X$ similar to the example with $\beta$ above.

*I think* a fuller expression for the Bayesian ERGM would be

$$P(X_{mis} | X_{obs}, D, \theta, \eta) = \frac{P(X_{mis}, X_{obs} | \theta)P(D|X, \eta) P(\theta)P(\eta)}{\sum_i P(X_{obs}, D, \theta, \eta | X_{mis} = i)}$$


## Some questions?

**What do we gain for modelling missing ties?**

Is it ultimately for imputation purposes? If we have some information about the missing tie mechanism, does that inform us of a better (multiple) imputation specification?

**How to use a missing tie model?**

Not entirely sure. If we know how the missing ties are generated, then we might apply that model to predict what their potential observable values were. e.g., $X_{mis} | X_{obs}, \theta$

*Tiny udpate?:* if we assume the response mechanism to be nonignorable, we can specify different models for missing data and observed data.

**Are all missing ties equal?**

While some missing ties can be missing for different reasons, we can't tell since they're missing. Unless there's some survey design information (e.g., we know the survey has a systematic pattern of missingness because certain parts of the survey were specified incorrectly).

**TODO:** this sounds like I'm rethinking multiple imputation, read up on it.

**How does this document serve as a proof of concept?**

I think this document demonstrates my ability in generating random graphs, degrading them, and making comparisons between the observed and degraded graphs.

I think, to a certain degree, a lot of the code in this document can be applied to missing data indicators by substituting the generated graph with a missing data indicator matrix.

**If I were omniscient and knew how to describe missing data mechanisms, what would I be able to tell and how can I affect the results?**

If I knew how the data were missing, or more specifically knew how to specify the not-at-random systematic-ness under the MNAR assumption, that would essentially be another statistical model to describe systematic-ness, but with missing data.

Somewhat ideally, I think if we did have a model to systematically explain why some data points were missing, we could then partial out the systematic missing bits (e.g., maybe variables $A-C$ didn't contribute to missingness but variables $D-F$ did). If so could we then model it so we evaluate $P(X|A-C, D-F = 0)$ holding $D-F$ constant so we can still reasonably impute variables for $A-C$? 

tl;dr, I think that describing *why* datapoints are missing is useful in explaining why other data points may not be missing (given the MNAR assumption).

In attempt to describe this with notation, say we knew

$$P(D|X, \eta)$$

This would mean that we knew the outcomes of D and explain D's realisations $d_{ij} = 0/1$ in relation to $X$ and $\eta$.

Then I think it would mean that we could

$$X_{mis} \sim D|X, \eta$$

so we could try evaluate $P(X_{mis}, X_{obs}|\theta)$

where $\theta$ can estimate the parts of $X_{mis}$ that are independent to $\eta$.

The idea here's that if we knew what made the missing data points missing, we could try partialling them out by making sure $\theta$ has no overlap with $\eta$, which is only possible if we knew the model parameters $\eta$ to model missing data realisations in $D$.

In the opposite end, what if we didn't know $P(D|X, \eta)$?

I'd think $X_{mis}$ would be a problem because it's full of unknown and undefined (*not unobserved*) values that we don't know how or why they're missing. While quick methods for dealing with them can be easy to apply (e.g., ignoring missing portions, imputation via mean), the lack of specification without $P(D|X, \eta)$ means that whatever method is applied to treat the missing data will not be 'systematic' in the sense of modelling.

Note that if we could sufficiently assume **MAR**, this is largely unnecessary since you know you've observed the mechanism for missingness and can just draw from the same model.

$$P(X| X_{obs}, \theta, D = 1) = P(X|X_{obs}, \theta, D = 0)$$

i.e., Ignorability seems like a very convenient property.

**How is imputation different from prediction?**

To me, it does feel like we're doing a very um... standard(?) statistical approach to modelling. We're quite literally just trying to describe parameters that align with a particular outcome. What's so different here?

I think a key difference is that we're dealing with *missing* values. Missing values are by undefinable. While we may know their possible outcomes, missing values are inherently undefined (i.e., we're never certain that they will be observed or unobserved).

Prediction's an unfortunately vague term, because I think you can describe it as some kind of partial prediction if you take prediction to be equivalent to drawing things from a posterior...

I think there's still some finer points regarding this comparison though. Imputations usually are made to fill in gaps of information (i.e., maybe variables $A-C, E-F$ are observed, but $D$ is not observed, we can fill in $D|A-C, E, F$). Predictions, still unfortunately a vague term, exist to generate possible values given a pre-specified condition. In that sense, I think the two are conceptually different as imputation exists to patch up missing holes in the data while prediction is more generally about generating some value given some condition. I think some imputations can involve some predictions, but I think it's clear that not all predictions are imputations.


**What's next?**

Given the bank of covert network data, I think an interesting avenue would be to model them to see if there's any sense to the madness (e.g., ideally a consistent network effect given the covert network's context?). In an ideal world, any predictions I had would be backed up by some literature. The Mitchell Centre paper on covert networks had a nice table of context and predictions. 

Another interesting avenue would be attempting to model the missing ties, although I'm not entirely sure how missing ties are indicated in the data...

I'm not sure how to handle unit nonresponse. I think making sense of a controlled unknown (i.e., with missingness data indicators with known dimensions) is markedly simpler than an unknown unknown (i.e., not sure how many people are in a covert network).


